{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, torch, argparse\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence,pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, os, pickle\n",
    "from scripts.pre_process import *\n",
    "import os, shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_inputs():\n",
    "    def __init__(self,data_dir=None, output_dir=None, meta_data=None,\n",
    "                 dataset_size=None, batch_size=100, num_epochs=None,\n",
    "                 project_path=None):\n",
    "        \n",
    "        self.data_dir=data_dir\n",
    "        self.output_dir=output_dir\n",
    "        self.dataset_size=dataset_size\n",
    "        self.batch_size=batch_size\n",
    "        self.meta_data = meta_data\n",
    "        self.num_epochs = num_epochs\n",
    "        self.project_path = project_path\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../image_captioning_pytorch/data/toy_dataset/'\n",
    "output_dir = 'smallest_test/'\n",
    "batch_size = 20\n",
    "meta_data = '../../image_captioning_pytorch/data/toy_dataset/toy_dataset_label.csv'\n",
    "dataset_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_inputs(data_dir=data_dir,output_dir=output_dir,meta_data=meta_data, batch_size=batch_size, \n",
    "                  dataset_size=dataset_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def reset_everything(path):\n",
    "    try:\n",
    "        folder = path\n",
    "        for filename in tqdm(os.listdir(folder)):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print('I do not think this folder exists: {}'.format(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset_everything(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../image_captioning_pytorch/data/toy_dataset/toy_dataset_label.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(args, output, pretrained):\n",
    "\n",
    "    df = pd.read_csv(args.meta_data, sep='\\t')\n",
    "\n",
    "    if os.path.exists(args.output_dir) == False:\n",
    "        os.mkdir(args.output_dir)\n",
    "\n",
    "        image_folder = '{}images/'.format(args.output_dir)\n",
    "        os.mkdir(image_folder)\n",
    "\n",
    "\n",
    "    image_folder = '{}images/'.format(args.output_dir)\n",
    "    \n",
    "    pre_processed_images = []\n",
    "    processed_images = []\n",
    "    captions = []\n",
    "    image_paths = []\n",
    "    failed_path = []\n",
    "    \n",
    "    print('Processing Artwork\\n')\n",
    "\n",
    "    for num, i in tqdm(enumerate(zip(df['FILE'], df['TITLE'])), total=args.dataset_size):\n",
    "        \n",
    "        if num == args.dataset_size:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            \n",
    "                \n",
    "            image = Image.open('{}{}'.format(args.data_dir, i[0]))\n",
    "            \n",
    "            resized_image = pre_process(image, pretrained)\n",
    "            \n",
    "            pre_processed_images.append(image)\n",
    "            processed_images.append(resized_image)\n",
    "            \n",
    "            captions.append(i[1])\n",
    "            \n",
    "            if args.output_dir and output == True:\n",
    "                \n",
    "                img2 = reconstruct_image(resized_image)\n",
    "                img2.save('{}{}'.format(image_folder, i[0]))\n",
    "                image_paths.append('{}{}'.format(image_folder,i[0]))\n",
    "\n",
    "            else:\n",
    "\n",
    "                image_paths.append('{}{}'.format(args.data_dir,i[0]))\n",
    "\n",
    "        except Exception as e:\n",
    "            print('something Wrong: {}'.format(e))\n",
    "            failed_path.append('{}{}'.format(args.data_dir,i[0]))\n",
    "\n",
    "            continue\n",
    "        \n",
    "    print('\\n{} artworks added to dataset'.format(len(processed_images)))\n",
    "    print('{} failed to load\\n'.format(len(failed_path)))\n",
    "\n",
    "    return processed_images,captions, image_paths, pre_processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize images for Resnet pretrained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Artwork\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ab8b1119954239a074e6ed27d5d5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "something Wrong: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]\n",
      "\n",
      "\n",
      "90 artworks added to dataset\n",
      "10 failed to load\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images, captions, image_paths, preprocessed_images = resize_images(args, output=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Vocab\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e60bfb874e41898766a7af4f559e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "173 tokens in vocab\n",
      "\n",
      "Outputting vocab object to smallest_test/vocab/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenized_titles = [tokenize(i) for i in captions]\n",
    "vocab = gen_vocab(tokenized_titles) \n",
    "print('\\nOutputting vocab object to {}'.format(pickle_data(vocab, args.output_dir, 'vocab')))\n",
    "encoded_titles, title_lengths = encode(tokenized_titles, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outputting dataloader object to smallest_test/dataloader_different_preprocessing/dataloader_different_preprocessing.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(images=images, captions=encoded_titles, image_paths=image_paths)\n",
    "train_dataloader = DataLoader(dataset, batch_size = args.batch_size)\n",
    "print('\\nOutputting dataloader object to {}'.format(pickle_data(train_dataloader, args.output_dir, 'dataloader_different_preprocessing')))\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(data_sampler):\n",
    "    \n",
    "    image, caption, i = next(data_sampler)\n",
    "    image.shape\n",
    "    plt.imshow(image.squeeze(0).permute(1,2,0))\n",
    "    plt.title('Sample Image')\n",
    "    plt.show()\n",
    "    \n",
    "    original_caption = decode_text(caption.squeeze(0).numpy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-11337ae8c839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-4593387ad80d>\u001b[0m in \u001b[0;36msample_dataset\u001b[0;34m(data_sampler)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_dataset(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset, batch_size = 1 ,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, argparse\n",
    "from scripts.models import EncoderCNN, DecoderRNN\n",
    "import torch, math\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from scripts.train import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, args, vocab, num_epochs):\n",
    "\n",
    "    encoder_model = EncoderCNN(300)\n",
    "    decoder_model = DecoderRNN(embed_size=300, hidden_size=512, vocab_size=len(vocab))\n",
    "\n",
    "    device = 'cpu'#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder_model.to(device)\n",
    "    decoder_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "    params = list(decoder_model.parameters()) + list(encoder_model.embed.parameters())\n",
    "\n",
    "    total_step = math.ceil(len(train_dataloader.dataset.caption_lengths) / train_dataloader.batch_sampler.batch_size)\n",
    "\n",
    "    optimizer = torch.optim.Adam(encoder_model.parameters(), lr=0.01)\n",
    "\n",
    "    encoder_model.train()\n",
    "    decoder_model.train()\n",
    "    vocab_size = len(vocab)\n",
    "    num = 1\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs+1),total=num_epochs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in tqdm(train_dataloader):\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                image = i[0].to(device)\n",
    "                caption = i[1].to(device)\n",
    "\n",
    "                decoder_model.zero_grad()\n",
    "                encoder_model.zero_grad()\n",
    "\n",
    "\n",
    "                features = encoder_model(image)\n",
    "                outputs = decoder_model(features, caption)\n",
    "\n",
    "                loss = criterion(outputs.view(-1, vocab_size), caption.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                num+=1 \n",
    "            except Exception as e:\n",
    "                failed_batch += 1\n",
    "                continue\n",
    "                \n",
    "                \n",
    "        print('Loss after epoch {}: {}'.format(epoch, loss))\n",
    "\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args=arg_inputs(num_epochs=50, project_path='medium_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab, dataloader = load_objects(train_args.project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder, trained_decoder = train(train_dataloader, train_args, vocab, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_encoder = trained_encoder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_obj(dataset_tuple):\n",
    "\n",
    "    test_image = dataset_tuple[0]\n",
    "    test_caption = decode_text(dataset_tuple[1].numpy())\n",
    "    \n",
    "    return test_image, test_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(list_nums):\n",
    "    decode_vocab = {num: word for word, num in vocab.items()}\n",
    "    return ' '.join([decode_vocab[i] for i in list_nums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "test_encoder.cuda()\n",
    "test_decoder.cuda()\n",
    "\n",
    "for i in tqdm(dataset[:20]):\n",
    "    \n",
    "    image, caption = get_test_obj(i)\n",
    "    \n",
    "    image = image.cuda()\n",
    "    features = trained_encoder(image.unsqueeze(0))\n",
    "    output = trained_decoder.sample(features.unsqueeze(1))\n",
    "    \n",
    "    cleaned_text = decode_text(output)\n",
    "    \n",
    "    titles.append((cleaned_text, caption))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.imshow(dataset[3][0].permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(train_dataloader, title):\n",
    "    \n",
    "    \n",
    "    orig_image, caption, path = next(train_dataloader)\n",
    "    image = orig_image.cuda()\n",
    "    plt.imshow(orig_image.squeeze(0).permute(1,2,0))\n",
    "    plt.title('Sample Image')\n",
    "    #plt.show()\n",
    "    features = trained_encoder(image.cpu()).unsqueeze(0)\n",
    "    output = trained_decoder.sample(features)    \n",
    "    sentence = decode_text(output)\n",
    "    original_caption = decode_text(caption.squeeze(0).numpy())\n",
    "    \n",
    "    title.append((sentence,original_caption))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "\n",
    "for i in tqdm(range(0,5000),total=5000):\n",
    "    get_prediction(it, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([i[0] for i in title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
